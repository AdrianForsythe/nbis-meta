## PATHS
workdir: .
# Sample information file
sample_list: samples/example_sample_list.tsv
# Main folder for results
results_path: results
# Subfolder for report files
report_path: results/report
# Path to store intermediate files
intermediate_path: results/intermediate
# Temporary paths
temp_path: temp
scratch_path: temp
# Path to store resource files
resource_path: resources

## PREPROCESSING
# Run Fastqc on preprocessed reads?
fastqc: True
# Run Trimmomatic?
trimmomatic: True
# Also trim adapters (in addition to quality trimming)?
trim_adapters: True
# Adapter type to trim from paired end libraries with trimmomatic
# ["NexteraPE-PE", "TruSeq2-PE", "TruSeq3-PE", "TruSeq3-PE-2"]
trimmomatic_pe_adapter: "TruSeq3-PE-2"
# Adapter type to trim from single end libraries with trimmomatic
# ["TruSeq2-SE", "TruSeq3-SE"]
trimmomatic_se_adapter: "TruSeq3-SE"
# Trimmomatic parameters for trimming adapters on paired-end samples
pe_adapter_params: "2:30:15"
# Trimmomatic parameters for trimming adapters on single-end samples
se_adapter_params: "2:30:15"
# Trimmomatic parameters for trimming prior to adapter removal on paired-end samples
pe_pre_adapter_params: ""
# Trimmomatic parameters for trimming after adapter removal on paired-end samples
pe_post_adapter_params: "LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:31"
# Trimmomatic parameters for trimming prior to adapter removal on single-end samples
se_pre_adapter_params: ""
# Trimmomatic parameters for trimming after adapter removal on single-end samples
se_post_adapter_params: "LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:31"

# Run cutadapt? (runs instead of Trimmomatic if True)
cutadapt: False
# Adapter sequence to trim with cutadapt
# Shown here is for Illumina TruSeq Universal Adapter.
adapter_sequence: AGATCGGAAGAGCACACGTCTGAACTCCAGTCA
# Reverse adapter sequence to trim with cutadapt
rev_adapter_sequence: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT
# Maximum allowed error rate as value between 0 and 1 (no. of errors divided by length of matching region)
cutadapt_error_rate: 0.1 #Maximum allowed error rate as value between 0 and 1

# Run fastuniq (removes duplicates from paired-end samples)
fastuniq: False
# Filter samples against the phiX genome?
phix_filter: True
# Run SortMeRNA to identify (and filter) rRNA sequences
sortmerna: False
# Sortmerna produces files with reads aligning to rRNA ('rRNA' extension)
# and not aligning to rRNA ('non_rRNA') extension
# Which reads should be used for downstream analyses
sortmerna_keep: 'non_rRNA'
# Remove filtered reads (i.e. the reads NOT specified in 'keep:')
sortmerna_remove_filtered: False
# Databases to use for rRNA identification
sortmerna_dbs: ["rfam-5s-database-id98.fasta","rfam-5.8s-database-id98.fasta","silva-arc-16s-id95.fasta","silva-arc-23s-id98.fasta","silva-bac-16s-id90.fasta","silva-bac-23s-id98.fasta","silva-euk-18s-id95.fasta","silva-euk-28s-id98.fasta"]
# Put both paired reads into rRNA bin (paired_in) or both reads in other bin (paired_out)
sortmerna_paired_strategy: "paired_in"
# Additional parameters for sortmerna
sortmerna_params: "--num_alignments 1"

## POSTPROCESSING
# Run MarkDuplicates to remove duplicates post mapping
markduplicates: True

## ASSEMBLY
# Number of threads to use for assembler
assembly_threads: 20
# Run Megahit assembler?
megahit: True
# Keep intermediate contigs from Megahit?
megahit_keep_intermediate: False
# Additional settings for Megahit
megahit_additional_settings: '--min-contig-len 300 --prune-level 3'
# Use Metaspades instead of Megahit for assembly?
metaspades: False
# Keep intermediate contigs from Metaspades?
metaspades_keep_intermediate: False
# Keep corrected reads produced during Metaspades assembly?
metaspades_keep_corrected: True
# Additional settings for Metaspades
metaspades_additional_settings: '-k 21,31,41,51,61,71,81,91,101,111,121'

## ANNOTATION
# Run tRNAscan-SE?
tRNAscan: True
# Run infernal for rRNA identification?
infernal: True
# Threads to use for infernal
infernal_threads: 8
# Where to store infernal database
infernal_dbpath: resources/infernal
# Run eggnog-mapper?
eggnog: True
# Run PFAM-scan?
pfam: True
# Run Resistance gene identifier?
rgi: False
# Parameters for rgi
rgi_params: "-a diamond --local --clean --input_type protein"
# Run taxonomic annotation of assembled contigs (using tango + sourmash)?
taxonomic_annotation: True
# Minimum length of contigs to use for taxonomic annotation
taxonomy_min_len: 500
# Parameters for tango search
tango_search_params: "--evalue 0.01 --top 10"
# Parameters for tango assigner
tango_assign_params: "--evalue 0.001 --top 5"
# Ranks to report taxonomy for
taxonomy_ranks: ["superkingdom","phylum","class","order","family","genus","species"]
# Protein database to use for taxonomic assignments
# Choose between uniref50, uniref90, uniref100 and nr.
taxdb: uniref100
# Threads to use for diamond
diamond_threads: 20

## BINNING
# Minimum length of contigs to use for binning.
# Enter more values to run binning multiple times.
min_contig_length: [1500]
# Run Metabat2 binner
metabat: False
# Threads for Metabat2
metabat_threads: 20
# Run CONCOCT binner
concoct: False
# Threads for concoct
concoct_threads: 20
# Run Checkm to assess quality of bins?
checkm: False
# Run checkm taxonomy wf instead of lineage wf for bin QC
checkm_taxonomy_wf: False
# Rank to use for checkm taxonomy wf
checkm_rank: life
# Taxon to use for checkm taxnomy wf
checkm_taxon: Prokaryote
# Use a reduced pplacer tree for checkm (uses less RAM)
checkm_reduced_tree: False
# Run gtdbtk to classify bins phylogenetically
gtdbtk: False

## MAPPING
bowtie2_threads: 10
bowtie2_params: "--very-sensitive"

## CLASSIFY READS
# Run kraken2 read classifier?
kraken: False
# Generate the standard kraken database?
kraken_standard_db: False
# Download a prebuilt kraken2 database from the CCB servers
# Choose from 'minikraken_8GB','16S_Greengenes','16S_RDP','16S_Silva'
kraken_prebuilt: "minikraken_8GB"
# Specify a path to an existing kraken2 database
kraken_custom: ""
# Run kraken2 in '--memory-mapping' mode which avoids loading database into RAM and uses less memory
kraken_reduce_memory: False
# Run centrifuge to classify reads
centrifuge: False
# Download a prebuilt centrifuge index to download from CCB servers
# Choose from "p+h+v", "nt_2018_2_12", "nt_2018_3_3", "p_compressed+h+v", "p_compressed_2018_4_15"
centrifuge_prebuilt: 'p_compressed+h+v'
# Specify a path to an existing centrifuge database
centrifuge_custom: ""
# Minimum score for classifications by centrifuge.
# Because centrifuge doesn't have a filtering algorithm, we use this min_score to filter results.
centrifuge_min_score: 75
# Maximum number of assignments per read
# By default this is set to 5 in centrifuge, increase to show more specific assignments
# Set to 1 to implement LCA-classification as in Kraken
centrifuge_max_assignments: 1
# Run metaphlan profiler?
metaphlan: False
# Version of the metaphlan database to use
metaphlan_index: "mpa_v30_CHOCOPhlAn_201901"