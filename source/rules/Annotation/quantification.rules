localrules:
    write_featurefile,
    normalize_featurecount,
    aggregate_featurecount,
    sum_to_taxa,
    sum_to_features,
    normalize_pathways_modules,
    taxonomy2krona

rule write_featurefile:
    input:
        opj(config["results_path"],"annotation","{group}","final_contigs.gff")
    output:
        opj(config["results_path"],"annotation","{group}","final_contigs.features.gff")
    shell:
        """
        python source/utils/parse_prodigal_gff.py {input} > {output}
        """

rule featurecount_pe:
    input:
        gff=opj(config["results_path"],"annotation","{group}","final_contigs.features.gff"),
        bam=opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_pe"+POSTPROCESS+".bam")
    output:
        opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_pe.fc.tab"),
        opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_pe.fc.tab.summary")
    threads: 4
    params: tmpdir = config["tmpdir"]
    resources:
        runtime = lambda wildcards, attempt: attempt**2*30
    shell:
        """
        featureCounts -a {input.gff} -o {output[0]} -t CDS -g gene_id -M -p -B -T {threads} --tmpDir {params.tmpdir} {input.bam}
        """

rule featurecount_se:
    input:
        gff=opj(config["results_path"],"annotation","{group}","final_contigs.features.gff"),
        bam=opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_se"+POSTPROCESS+".bam")
    output:
        opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_se.fc.tab"),
        opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_se.fc.tab.summary")
    threads: 4
    params: tmpdir = config["tmpdir"]
    resources:
        runtime = lambda wildcards, attempt: attempt**2*30
    shell:
        """
        featureCounts -a {input.gff} -o {output[0]} -t CDS -g gene_id -M -T {threads} --tmpDir {params.tmpdir} {input.bam}
        """

rule normalize_featurecount:
    input:
        opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_{seq_type}.fc.tab"),
        opj(config["intermediate_path"],"preprocess","read_lengths.tab")
    output:
        opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_{seq_type}.fc.tpm.tab"),
        opj(config["results_path"],"assembly","{group}","mapping","{sample}_{run}_{seq_type}.fc.raw.tab")
    params:
        s = "{sample}_{run}",
        script = "source/utils/featureCountsTPM.py"
    shell:
        """
        rl=$(grep -w {params.s} {input[1]} | cut -f2)
        python {params.script} --rl $rl -i {input[0]} -o {output[0]} --rc {output[1]} --sampleName {params.s}
        """

def get_fc_files(wildcards, file_type):
    g = wildcards.group
    files = []
    for sample in assemblyGroups[g].keys():
        for run in assemblyGroups[g][sample].keys():
            if "se" in assemblyGroups[g][sample][run].keys():
                files.append(opj(config["results_path"],"assembly",g,"mapping",sample+"_"+run+"_se.fc.{}.tab".format(file_type)))
            else:
                files.append(opj(config["results_path"],"assembly",g,"mapping",sample+"_"+run+"_pe.fc.{}.tab".format(file_type)))
    return files

def concat_files(files, gff_df):
    df = pd.DataFrame()
    for f in files:
        _df = pd.read_csv(f, index_col=0, sep="\t")
        df = pd.concat([df,_df], axis=1)
    df = pd.merge(df, gff_df, left_index=True, right_on="gene_id")
    df.drop("gene_id", axis=1, inplace=True)
    df.set_index("orf", inplace=True)
    return df

rule aggregate_featurecount:
    """Aggregates feature count files and performs TPM normalization"""
    input:
        raw_files = get_all_files(samples, opj(config["results_path"], "assembly", "{group}", "mapping"), ".fc.raw.tab"),
        tpm_files = get_all_files(samples, opj(config["results_path"], "assembly", "{group}", "mapping"), ".fc.tpm.tab"),
        gff_file = opj(config["results_path"],"annotation","{group}","final_contigs.features.gff")
    output:
        raw = opj(config["results_path"],"annotation","{group}","fc.count.tab"),
        tpm = opj(config["results_path"],"annotation","{group}","fc.tpm.tab")
    run:
        gff_df = pd.read_csv(input.gff_file, header=None, usecols=[0,8], names=["contig","gene"], sep="\t")
        gff_df = gff_df.assign(gene_id=pd.Series([x.replace("gene_id ","") for x in gff_df.gene], index=gff_df.index))
        gff_df = gff_df.assign(suffix=pd.Series([x.split(" ")[-1].split("_")[-1] for x in gff_df.gene],index=gff_df.index))
        gff_df = gff_df.assign(orf=pd.Series(gff_df.contig+"_"+gff_df.suffix, index=gff_df.index))
        gff_df = gff_df[["orf","gene_id"]]

        raw_df = concat_files(input.raw_files, gff_df)
        tpm_df = concat_files(input.tpm_files, gff_df)
        raw_df.to_csv(output.raw, sep="\t")
        tpm_df.to_csv(output.tpm, sep="\t")

def process_and_sum(q_df,annot_df):
    # Merge annotations and abundance, keep ORFs without annotation as "Unclassified"
    annot_q_df = pd.merge(annot_df, q_df, left_index = True, right_index = True, how="right")
    annot_q_df.fillna("Unclassified", inplace=True)
    feature_cols = annot_df.columns
    annot_q_sum = annot_q_df.groupby(list(feature_cols)).sum().reset_index()
    annot_q_sum.set_index(feature_cols[0], inplace=True)
    return annot_q_sum

rule sum_to_features:
    input:
        count = opj(config["results_path"],"annotation","{group}","fc.count.tab"),
        norm = opj(config["results_path"],"annotation","{group}","fc.tpm.tab"),
        annot_file = opj(config["results_path"],"annotation","{group}","{db}.parsed.tab")
    output:
        count = opj(config["results_path"],"annotation","{group}","{db}.parsed.count.tab"),
        norm = opj(config["results_path"],"annotation","{group}","{db}.parsed.tpm.tab")
    run:
        import pandas as pd
        annot_df = pd.read_csv(input.annot_file, index_col=0, sep="\t")
        norm_df = pd.read_csv(input.norm, index_col=0, sep="\t")
        count_df = pd.read_csv(input.count, index_col=0, sep="\t")
        feature_norm_sum = process_and_sum(norm_df,annot_df)
        feature_count_sum = process_and_sum(count_df,annot_df)
        feature_norm_sum.to_csv(output.norm, sep="\t")
        feature_count_sum.to_csv(output.count, sep="\t")

rule normalize_pathways_modules:
    """Normalizes pathway and module abundances by the size of each pathway/module"""
    input:
        count = opj(config["results_path"],"annotation","{group}","{db}.parsed.count.tab"),
        norm = opj(config["results_path"],"annotation","{group}","{db}.parsed.tpm.tab"),
        annot_file = opj(config["results_path"],"annotation","{group}","{db}.parsed.tab"),
        info = opj(config["resource_path"],"kegg","kegg_ko2{db}.tsv")
    output:
        count = opj(config["results_path"],"annotation","{group}","{db}.parsed.count.normalized.tab"),
        norm = opj(config["results_path"],"annotation","{group}","{db}.parsed.tpm.normalized.tab")
    shell:
        """
        python source/utils/eggnog-parser.py quantify {input.count} {input.annot_file} {output.count}\
         --normalize {input.info}
         python source/utils/eggnog-parser.py quantify {input.norm} {input.annot_file} {output.norm}\
         --normalize {input.info}
        """

rule sum_to_taxa:
    input:
        tax = opj(config["results_path"],"annotation","{group}","taxonomy",
            "orfs.{db}.taxonomy.tsv".format(db=config["taxdb"])),
        count = opj(config["results_path"],"annotation","{group}","fc.count.tab"),
        norm = opj(config["results_path"],"annotation","{group}","fc.tpm.tab")
    output:
        count = opj(config["results_path"],"annotation","{group}","taxonomy","tax.count.tab"),
        norm = opj(config["results_path"],"annotation","{group}","taxonomy","tax.tpm.tab")
    run:
        header = ["protein","superkingdom", "phylum","class","order","family","genus","species"]
        df = pd.read_csv(input.tax, sep="\t", index_col=0, header=None, names=header)
        count_df = pd.read_csv(input.count, header=0, index_col=0, sep="\t")
        norm_df = pd.read_csv(input.norm, header=0, index_col=0, sep="\t")

        taxa_count = pd.merge(df,count_df,right_index=True,left_index=True)
        taxa_count_sum = taxa_count.groupby(header[1:]).sum().reset_index()
        taxa_count_sum.to_csv(output.count, sep="\t", index=False)

        taxa_norm = pd.merge(df,norm_df,right_index=True,left_index=True)
        taxa_norm_sum = taxa_norm.groupby(header[1:]).sum().reset_index()
        taxa_norm_sum.to_csv(output.norm, sep="\t", index=False)

rule taxonomy2krona:
    input:
        opj(config["results_path"],"annotation","{group}","taxonomy","tax.tpm.tab"),
        opj(config["results_path"],"annotation","{group}","taxonomy","tax.count.tab")
    output:
        opj(config["results_path"],"annotation","{group}","taxonomy","taxonomy.tpm.krona.html"),
        opj(config["results_path"],"annotation","{group}","taxonomy","taxonomy.count.krona.html")
    params: temp_dir = os.path.expandvars(config["temp_path"])
    run:
        shell("mkdir -p {params.temp_dir}")
        inputs = make_krona_taxonomy_input(input[0], params.temp_dir)
        input_string = " ".join(inputs)
        shell("ktImportText -o {output[0]} {input_string}")
        for f in [item.split(",")[0] for item in inputs]:
            shell("rm {f}")

        inputs = make_krona_taxonomy_input(input[1], params.temp_dir)
        input_string = " ".join(inputs)
        shell("ktImportText -o {output[1]} {input_string}")
        for f in [item.split(",")[0] for item in inputs]:
            shell("rm {f}")